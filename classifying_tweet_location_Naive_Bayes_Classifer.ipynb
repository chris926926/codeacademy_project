{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-Platform Project: Classifying Tweets\n",
    "\n",
    "In this off-platform project, you will use a Naive Bayes Classifier to find patterns in real tweets. We've given you three files: `new_york.json`, `london.json`, and `paris.json`. These three files contain tweets that we gathered from those locations.\n",
    "\n",
    "The goal is to create a classification algorithm that can classify any tweet (or sentence) and predict whether that sentence came from New York, London, or Paris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate the Data\n",
    "\n",
    "import the data and printed the following information:\n",
    "* The number of tweets.\n",
    "* The columns, or features, of a tweet.\n",
    "* The text of the 12th tweet in the New York dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4723\n",
      "Index(['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
      "       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',\n",
      "       'favorited', 'filter_level', 'geo', 'id', 'id_str',\n",
      "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
      "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
      "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',\n",
      "       'possibly_sensitive', 'quote_count', 'quoted_status',\n",
      "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',\n",
      "       'reply_count', 'retweet_count', 'retweeted', 'source', 'text',\n",
      "       'timestamp_ms', 'truncated', 'user', 'withheld_in_countries'],\n",
      "      dtype='object')\n",
      "Be best #ThursdayThoughts\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_york_tweets = pd.read_json(\"new_york.json\", lines=True)\n",
    "print(len(new_york_tweets))\n",
    "print(new_york_tweets.columns)\n",
    "print(new_york_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, load the London and Paris tweets into DataFrames named `london_tweets` and `paris_tweets`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341\n",
      "Index(['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
      "       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',\n",
      "       'favorited', 'filter_level', 'geo', 'id', 'id_str',\n",
      "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
      "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
      "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',\n",
      "       'possibly_sensitive', 'quote_count', 'quoted_status',\n",
      "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',\n",
      "       'reply_count', 'retweet_count', 'retweeted', 'source', 'text',\n",
      "       'timestamp_ms', 'truncated', 'user'],\n",
      "      dtype='object')\n",
      "I saw this on the BBC and thought you should see it:\n",
      "\n",
      "The precious metal sparking a new gold rush - https://t.co/ScW4MOSobZ\n"
     ]
    }
   ],
   "source": [
    "# import londa tweets\n",
    "london_tweets= pd.read_json(\"london.json\", lines=True)\n",
    "print(len(london_tweets))\n",
    "print(london_tweets.columns)\n",
    "print(london_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2510\n",
      "Index(['contributors', 'coordinates', 'created_at', 'display_text_range',\n",
      "       'entities', 'extended_entities', 'extended_tweet', 'favorite_count',\n",
      "       'favorited', 'filter_level', 'geo', 'id', 'id_str',\n",
      "       'in_reply_to_screen_name', 'in_reply_to_status_id',\n",
      "       'in_reply_to_status_id_str', 'in_reply_to_user_id',\n",
      "       'in_reply_to_user_id_str', 'is_quote_status', 'lang', 'place',\n",
      "       'possibly_sensitive', 'quote_count', 'quoted_status',\n",
      "       'quoted_status_id', 'quoted_status_id_str', 'quoted_status_permalink',\n",
      "       'reply_count', 'retweet_count', 'retweeted', 'source', 'text',\n",
      "       'timestamp_ms', 'truncated', 'user'],\n",
      "      dtype='object')\n",
      "Hauts-de-Seine : l’incendie d’Issy-les-Moulineaux prive aussi 16 000 foyers de courant https://t.co/Hlb02Fpliy\n"
     ]
    }
   ],
   "source": [
    "# import paris tweets\n",
    "paris_tweets= pd.read_json(\"paris.json\", lines=True)\n",
    "print(len(paris_tweets))\n",
    "print(paris_tweets.columns)\n",
    "print(paris_tweets.loc[12][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying using language: Naive Bayes Classifier\n",
    "\n",
    "We're going to create a Naive Bayes Classifier! Let's begin by looking at the way language is used differently in these three locations. Let's grab the text of all of the tweets and make it one big list. In the code block below, we've created a list of all the New York tweets. Do the same for `london_tweets` and `paris_tweets`.\n",
    "\n",
    "Then combine all three into a variable named `all_tweets` by using the `+` operator. For example, `all_tweets = new_york_text + london_text + ...`\n",
    "\n",
    "Let's also make the labels associated with those tweets. `0` represents a New York tweet, `1`  represents a London tweet, and `2` represents a Paris tweet. Finish the definition of `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_york_text = new_york_tweets[\"text\"].tolist()\n",
    "london_text=london_tweets['text'].tolist()\n",
    "paris_text=paris_tweets['text'].tolist()\n",
    "all_tweets = new_york_text + london_text+paris_text\n",
    "labels = [0] * len(new_york_text) + [1]*len(london_text)+[2]*len(paris_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@DelgadoforNY19 Calendar marked.', 'petition to ban more than one spritz of cologne', 'People really be making up beef with you in they head lol', '30 years old.. wow what a journey... I moved to NYC at 22 young and dumb, without even $100 in my bank account and… https://t.co/awjzsvoGS7', 'At first glance it looked like asparagus with chicken and gravy smothered over it or potatoes. She gotta be extra w… https://t.co/InBNnsKuWu']\n",
      "[0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(all_tweets[:5])\n",
    "print(labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Training and Test Set\n",
    "\n",
    "We can now break our data into a training set and a test set. We'll use scikit-learn's `train_test_split` function to do this split. This function takes two required parameters: It takes the data, followed by the labels. Set the optional parameter `test_size` to be `0.2`. Finally, set the optional parameter `random_state` to `1`. This will make it so your data is split in the same way as the data in our solution code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data, train_labels,test_labels=train_test_split(all_tweets,labels,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10059\n",
      "2515\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Count Vectors\n",
    "\n",
    "To use a Naive Bayes Classifier, we need to transform our lists of words into count vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saying bye is hard. Especially when youre saying bye to comfort.\n",
      "  (0, 5022)\t2\n",
      "  (0, 6371)\t1\n",
      "  (0, 9552)\t1\n",
      "  (0, 12314)\t1\n",
      "  (0, 13903)\t1\n",
      "  (0, 23994)\t2\n",
      "  (0, 27146)\t1\n",
      "  (0, 29397)\t1\n",
      "  (0, 30274)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter=CountVectorizer()\n",
    "counter.fit(train_data)\n",
    "train_counts=counter.transform(train_data)\n",
    "test_counts=counter.transform(test_data)\n",
    "print(train_data[3])\n",
    "print(train_counts[3])   # saying appear twice, bye appear twice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test the Naive Bayes Classifier\n",
    "\n",
    "We now have the inputs to our classifier. Let's use the CountVectors to train and test the Naive Bayes Classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "classifier=MultinomialNB()\n",
    "classifier.fit(train_counts,train_labels)\n",
    "predictions=classifier.predict(test_counts)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Your Model\n",
    "\n",
    "Now that the classifier has made its predictions, let's see how well it did. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6779324055666004\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels,predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[541 404  28]\n",
      " [203 824  34]\n",
      " [ 38 103 340]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_labels,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Your Own Tweet\n",
    "\n",
    "The classifier predicts tweets that were actually from New York as either New York tweets or London tweets, but almost never Paris tweets. Similarly, the classifier rarely misclassifies the tweets that were actually from Paris. Tweets coming from two English speaking countries are harder to distinguish than tweets in different languages.\n",
    "\n",
    "Now let's write a tweet and see how the classifier works! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "tweet=\"I love coding so much, it's awesome!\"\n",
    "tweet_counts=counter.transform([tweet])\n",
    "print(classifier.predict(tweet_counts))   # our model predict it from London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "tweet=\"I live in Manhattan!\"\n",
    "tweet_counts=counter.transform([tweet])\n",
    "print(classifier.predict(tweet_counts))   # our model predict it from New York"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "tweet=\"I love Paris!\"\n",
    "tweet_counts=counter.transform([tweet])\n",
    "print(classifier.predict(tweet_counts))    # our model predict it from Paris!  Amazing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
